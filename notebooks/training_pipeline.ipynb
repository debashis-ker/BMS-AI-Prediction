{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e689d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def transform_data_with_columntransformer(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    This function takes a pandas DataFrame, applies a series of transformations\n",
    "    using ColumnTransformer, and returns features (X) and target variables (y).\n",
    "    \"\"\"\n",
    "\n",
    "    df['data_received_on'] = pd.to_datetime(df['data_received_on'])\n",
    "    df['data_received_on_naive'] = df['data_received_on'].dt.tz_localize(None)\n",
    "\n",
    "    df.sort_values('data_received_on_naive', inplace=True)\n",
    "\n",
    "    converted_df = df.pivot_table(\n",
    "        index=['data_received_on_naive', 'site', 'system_type'],\n",
    "        columns='datapoint',\n",
    "        values='monitoring_data',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    converted_df.reset_index(inplace=True)\n",
    "\n",
    "    numeric_cols = [\n",
    "        \"RA Damper feedback\", \"SA Pressure setpoint\", \"OA Humid\", \"RA Temp\",\n",
    "        \"RA CO2\", \"RA CO2 setpoint\", \"SA Fan Speed feedback\", \"SA Fan Speed control\",\n",
    "        \"RA Temp control( Valve Feedback)\", \"SA pressure\", \"Fan Power meter (KW)\",\n",
    "        \"RA damper control\", \"OA Temp\", \"OA Flow\", \"SA temp\", \"RA  temperature setpoint\"\n",
    "    ]\n",
    "    present_numeric_cols = [col for col in numeric_cols if col in converted_df.columns]\n",
    "    converted_df[present_numeric_cols] = converted_df[present_numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    cols_to_drop = [\n",
    "        'site', 'system_type', 'Bag filter dirty status', 'Plant enable',\n",
    "        'Trip status', 'airflow Status', 'auto Status', 'pre Filter dirty staus'\n",
    "    ]\n",
    "    converted_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "    if \"Sup fan cmd\" in converted_df.columns:\n",
    "        mappings = {'active': 1, 'inactive': 0}\n",
    "        converted_df[\"Sup fan cmd\"] = converted_df[\"Sup fan cmd\"].replace(mappings)\n",
    "\n",
    "    target_columns = [\"RA damper control\", \"RA Temp control( Valve Feedback)\", \"SA Fan Speed control\", \"Fan Power meter (KW)\"]\n",
    "    present_target_cols = [col for col in target_columns if col in converted_df.columns]\n",
    "    \n",
    "    converted_df.dropna(subset=present_target_cols, inplace=True)\n",
    "\n",
    "    y = converted_df[present_target_cols]\n",
    "    X = converted_df.drop(columns=present_target_cols, errors='ignore')\n",
    "\n",
    "    converted_df['hour'] = converted_df['data_received_on_naive'].dt.hour\n",
    "    converted_df['dayofweek'] = converted_df['data_received_on_naive'].dt.dayofweek\n",
    "    converted_df['month'] = converted_df['data_received_on_naive'].dt.month\n",
    "    converted_df['dayofyear'] = converted_df['data_received_on_naive'].dt.dayofyear\n",
    "    X = converted_df.drop(columns=['data_received_on_naive', 'data_received_on'], errors='ignore')\n",
    "    \n",
    "    numeric_features = [col for col in X.columns if pd.api.types.is_numeric_dtype(X[col]) and col != 'Sup fan cmd']\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', MinMaxScaler(), numeric_features)\n",
    "        ],\n",
    "        remainder='passthrough'  \n",
    "    )\n",
    "\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "\n",
    "    passthrough_cols = [col for col in X.columns if col not in numeric_features]\n",
    "    transformed_cols = numeric_features + passthrough_cols\n",
    "    X = pd.DataFrame(X_transformed, columns=transformed_cols, index=X.index)\n",
    "\n",
    "\n",
    "    combined = pd.concat([X, y], axis=1)\n",
    "    combined.dropna(inplace=True)\n",
    "    X = combined[X.columns]\n",
    "    y = combined[y.columns]\n",
    "\n",
    "\n",
    "    return X, y,preprocessor\n",
    "\n",
    "def load_and_preprocess_data(df : pd.DataFrame = None, file_path: str = 'C:/Users/debas/OneDrive/Desktop/output.csv'):\n",
    "    \"\"\"\n",
    "    Loads data, preprocesses it, and splits it into training and testing sets.\n",
    "    \"\"\"\n",
    "    print(\"--- Loading and Preprocessing Data ---\")\n",
    "    if not df:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: '{file_path}' not found. Please ensure the dataset is in the correct directory.\")\n",
    "            exit()\n",
    "    \n",
    "    \n",
    "    print(f\"Initial data shape: {df.shape}\")    \n",
    "    df = df[(df['site'] == \"Ground Floor\") & (df['system_type'] == \"AHU\")]\n",
    "    \n",
    "    X, y, preprocessor = transform_data_with_columntransformer(df)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "    \n",
    "    print(f\"Training data shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Testing data shape: X={X_test.shape}, y={y_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor\n",
    "\n",
    "# --- 2. MODEL TRAINING AND TUNING ---\n",
    "\n",
    "def tune_sklearn_models(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Defines and tunes scikit-learn models using RandomizedSearchCV.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Training for Scikit-learn Models ---\")\n",
    "    models_to_tune = {\n",
    "        # 'RandomForest': {\n",
    "        #     'estimator': RandomForestRegressor(random_state=42),\n",
    "        #     'params': {\n",
    "        #         'n_estimators': [100, 200, 300], 'max_depth': [10, 20, 30, None],\n",
    "        #         'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]\n",
    "        #     }\n",
    "        # },\n",
    "        # 'GradientBoosting': {\n",
    "        #     'estimator': GradientBoostingRegressor(random_state=42),\n",
    "        #     'params': {\n",
    "        #         'n_estimators': [100, 200, 300], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]\n",
    "        #     }\n",
    "        # },\n",
    "        'XGBoost': {\n",
    "            'estimator': xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300], 'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 5, 7], 'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_sklearn_models = {}\n",
    "    for name, config in models_to_tune.items():\n",
    "        print(f\"\\nTuning {name}...\")\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=config['estimator'], param_distributions=config['params'],\n",
    "            n_iter=10, cv=3, verbose=1, random_state=42, n_jobs=-1\n",
    "        )\n",
    "        search_wrapper = MultiOutputRegressor(random_search)\n",
    "        search_wrapper.fit(X_train, y_train)\n",
    "        best_sklearn_models[name] = search_wrapper\n",
    "        print(f\"Finished tuning {name}.\")\n",
    "    \n",
    "    return best_sklearn_models\n",
    "\n",
    "def create_keras_model_builder(input_shape, output_shape):\n",
    "    \"\"\"\n",
    "    Factory function to create the Keras model builder with specific input/output shapes.\n",
    "    \"\"\"\n",
    "    def build_model(hp):\n",
    "        inputs = keras.Input(shape=(input_shape,))\n",
    "        x = inputs\n",
    "        for i in range(hp.Int('num_layers', 1, 3)):\n",
    "            x = layers.Dense(\n",
    "                units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),\n",
    "                activation=hp.Choice('activation', ['relu', 'tanh'])\n",
    "            )(x)\n",
    "            x = layers.Dropout(hp.Float('dropout', 0, 0.5, step=0.1))(x)\n",
    "        outputs = layers.Dense(output_shape)(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=\"mean_squared_error\",\n",
    "            metrics=[\"mean_absolute_error\"],\n",
    "        )\n",
    "        return model\n",
    "    return build_model\n",
    "\n",
    "def tune_keras_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Tunes and trains a deep learning model using Keras Tuner.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Training for Deep Learning Model ---\")\n",
    "    model_builder = create_keras_model_builder(X_train.shape[1], y_train.shape[1])\n",
    "    tuner = kt.RandomSearch(\n",
    "        model_builder, objective='val_loss', max_trials=10, executions_per_trial=2,\n",
    "        directory='keras_tuner_dir', project_name='multi_output_regression'\n",
    "    )\n",
    "    tuner.search_space_summary()\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    print(\"\\nRunning Keras Tuner search...\")\n",
    "    tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    keras_model = tuner.get_best_models(num_models=1)[0]\n",
    "    print(f\"\\nOptimal Keras hyperparameters found: Learning Rate={best_hps.get('lr'):.4f}\")\n",
    "    return keras_model\n",
    "\n",
    "# --- 3. MODEL EVALUATION AND SAVING ---\n",
    "\n",
    "def evaluate_models(models: dict, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates a dictionary of trained models on the test set and returns a results DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Evaluating All Models on Test Set ---\")\n",
    "    evaluation_results = {}\n",
    "    for name, model in models.items():\n",
    "        predictions = model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        evaluation_results[name] = {'MAE': mae, 'MSE': mse, 'R2 Score': r2}\n",
    "        print(f\"\\n{name} Metrics: MAE={mae:.4f}, MSE={mse:.4f}, R2 Score={r2:.4f}\")\n",
    "        \n",
    "    results_df = pd.DataFrame(evaluation_results).T\n",
    "    print(\"\\n--- Model Comparison ---\")\n",
    "    print(results_df)\n",
    "    return results_df\n",
    "\n",
    "def save_best_model(best_model_name, best_model, preprocessor):\n",
    "    \"\"\"\n",
    "    Saves the best performing model and its preprocessor to disk.\n",
    "    \"\"\"\n",
    "    print(f\"\\nBest performing model is: {best_model_name}\")\n",
    "    if 'Keras' in best_model_name:\n",
    "        best_model.save('best_model.keras')\n",
    "        joblib.dump(preprocessor, 'preprocessor.joblib')\n",
    "        print(\"Saved Keras model to 'best_model.keras' and preprocessor to 'preprocessor.joblib'\")\n",
    "    else:\n",
    "        pipeline_to_save = {'preprocessor': preprocessor, 'model': best_model}\n",
    "        joblib.dump(pipeline_to_save, f'best_model_{best_model_name}.joblib')\n",
    "        print(f\"Saved complete pipeline to 'best_model_{best_model_name}.joblib'\")\n",
    "\n",
    "# --- 4. MAIN ORCHESTRATOR ---\n",
    "\n",
    "def main(data_file_path: str='', df: pd.DataFrame = None):\n",
    "    \"\"\"\n",
    "    Main function to run the entire training pipeline.\n",
    "    \"\"\"\n",
    "    # Step 1: Load and preprocess data\n",
    "    X_train, X_test, y_train, y_test, preprocessor = load_and_preprocess_data(data_file_path,df=df)\n",
    "\n",
    "    # Step 2: Train and tune models\n",
    "    best_sklearn_models = tune_sklearn_models(X_train, y_train)\n",
    "    keras_model = tune_keras_model(X_train, y_train)\n",
    "\n",
    "    # Step 3: Evaluate models\n",
    "    all_models = {**best_sklearn_models, 'Keras_Functional_API': keras_model}\n",
    "    results_df = evaluate_models(all_models, X_test, y_test)\n",
    "\n",
    "    # Step 4: Save the best model\n",
    "    best_model_name = results_df['R2 Score'].idxmax()\n",
    "    best_model = all_models[best_model_name]\n",
    "    save_best_model(best_model_name, best_model, preprocessor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f351d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62981c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/debas/OneDrive/Desktop/output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e917ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path to your dataset\n",
    "# DATA_FILE_PATH = 'C:/Users/debas/OneDrive/Desktop/output.csv'\n",
    "# main(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7ca1e9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0826bfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preprocessing Data ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23452\\2589814875.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDATA_FILE_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23452\\2138773454.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(df, file_path)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \"\"\"\n\u001b[0;32m    107\u001b[0m     \u001b[0mLoads\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocesses\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msplits\u001b[0m \u001b[0mit\u001b[0m \u001b[0minto\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtesting\u001b[0m \u001b[0msets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m    109\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--- Loading and Preprocessing Data ---\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\debas\\OneDrive\\Desktop\\BMS-AI\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1575\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1577\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1578\u001b[0m             \u001b[1;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1579\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1580\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, preprocessor = load_and_preprocess_data(df=df,file_path=DATA_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a2d5854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>service_id</th>\n",
       "      <th>asset_code</th>\n",
       "      <th>site</th>\n",
       "      <th>system_type</th>\n",
       "      <th>device_id</th>\n",
       "      <th>device_ip</th>\n",
       "      <th>object_name</th>\n",
       "      <th>equipment_name</th>\n",
       "      <th>equipment_id</th>\n",
       "      <th>data_received_on</th>\n",
       "      <th>datapoint</th>\n",
       "      <th>monitoring_data</th>\n",
       "      <th>service_status</th>\n",
       "      <th>subsystem</th>\n",
       "      <th>system_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FIR-FCU-01On/Off Cmd</td>\n",
       "      <td>FIR-FCU-01</td>\n",
       "      <td>First Floor</td>\n",
       "      <td>FCU</td>\n",
       "      <td>170101</td>\n",
       "      <td>192.168.170.101</td>\n",
       "      <td>First Floor\\\\FCU-1F-02 On/Off Cmd</td>\n",
       "      <td>FCU-1F-02</td>\n",
       "      <td>FCU-1F-02</td>\n",
       "      <td>2025-08-21T08:11:48.133 UTC</td>\n",
       "      <td>On/Off Cmd</td>\n",
       "      <td>active</td>\n",
       "      <td>normal</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FIR-FCU-01On/Off Cmd</td>\n",
       "      <td>FIR-FCU-01</td>\n",
       "      <td>First Floor</td>\n",
       "      <td>FCU</td>\n",
       "      <td>170101</td>\n",
       "      <td>192.168.170.101</td>\n",
       "      <td>First Floor\\\\FCU-1F-02 On/Off Cmd</td>\n",
       "      <td>FCU-1F-02</td>\n",
       "      <td>FCU-1F-02</td>\n",
       "      <td>2025-08-21T08:08:47.990 UTC</td>\n",
       "      <td>On/Off Cmd</td>\n",
       "      <td>active</td>\n",
       "      <td>normal</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FIR-FCU-01On/Off Cmd</td>\n",
       "      <td>FIR-FCU-01</td>\n",
       "      <td>First Floor</td>\n",
       "      <td>FCU</td>\n",
       "      <td>170101</td>\n",
       "      <td>192.168.170.101</td>\n",
       "      <td>First Floor\\\\FCU-1F-02 On/Off Cmd</td>\n",
       "      <td>FCU-1F-02</td>\n",
       "      <td>FCU-1F-02</td>\n",
       "      <td>2025-08-21T08:05:48.796 UTC</td>\n",
       "      <td>On/Off Cmd</td>\n",
       "      <td>active</td>\n",
       "      <td>normal</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FIR-FCU-01On/Off Cmd</td>\n",
       "      <td>FIR-FCU-01</td>\n",
       "      <td>First Floor</td>\n",
       "      <td>FCU</td>\n",
       "      <td>170101</td>\n",
       "      <td>192.168.170.101</td>\n",
       "      <td>First Floor\\\\FCU-1F-02 On/Off Cmd</td>\n",
       "      <td>FCU-1F-02</td>\n",
       "      <td>FCU-1F-02</td>\n",
       "      <td>2025-08-21T08:02:48.708 UTC</td>\n",
       "      <td>On/Off Cmd</td>\n",
       "      <td>active</td>\n",
       "      <td>normal</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FIR-FCU-01On/Off Cmd</td>\n",
       "      <td>FIR-FCU-01</td>\n",
       "      <td>First Floor</td>\n",
       "      <td>FCU</td>\n",
       "      <td>170101</td>\n",
       "      <td>192.168.170.101</td>\n",
       "      <td>First Floor\\\\FCU-1F-02 On/Off Cmd</td>\n",
       "      <td>FCU-1F-02</td>\n",
       "      <td>FCU-1F-02</td>\n",
       "      <td>2025-08-21T07:59:48.964 UTC</td>\n",
       "      <td>On/Off Cmd</td>\n",
       "      <td>active</td>\n",
       "      <td>normal</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14428292</th>\n",
       "      <td>GRO-FCU-05Ret temp</td>\n",
       "      <td>GRO-FCU-05</td>\n",
       "      <td>Ground Floor</td>\n",
       "      <td>FCU</td>\n",
       "      <td>170101</td>\n",
       "      <td>192.168.170.101</td>\n",
       "      <td>Ground Floor\\\\FCU\\\\FCU-GF-05 Ret temp</td>\n",
       "      <td>FCU-GF-05</td>\n",
       "      <td>FCU-GF-05</td>\n",
       "      <td>2025-05-23T12:57:34.673 UTC</td>\n",
       "      <td>Ret temp</td>\n",
       "      <td>41.72909164428711</td>\n",
       "      <td>{}</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14428293</th>\n",
       "      <td>GRO-FCU-05Ret temp</td>\n",
       "      <td>GRO-FCU-05</td>\n",
       "      <td>Ground Floor</td>\n",
       "      <td>FCU</td>\n",
       "      <td>170101</td>\n",
       "      <td>192.168.170.101</td>\n",
       "      <td>Ground Floor\\\\FCU\\\\FCU-GF-05 Ret temp</td>\n",
       "      <td>FCU-GF-05</td>\n",
       "      <td>FCU-GF-05</td>\n",
       "      <td>2025-05-23T12:54:28.494 UTC</td>\n",
       "      <td>Ret temp</td>\n",
       "      <td>40.109561920166016</td>\n",
       "      <td>{}</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14428294</th>\n",
       "      <td>ROO-CHI-63Secondary pump</td>\n",
       "      <td>ROO-CHI-63</td>\n",
       "      <td>Rooftop</td>\n",
       "      <td>Chiller system</td>\n",
       "      <td>170101</td>\n",
       "      <td>192.168.170.101</td>\n",
       "      <td>Rooftop\\\\Chiller system\\\\Pumps\\\\Secondary pump...</td>\n",
       "      <td>Pumps</td>\n",
       "      <td>Pumps</td>\n",
       "      <td>2025-05-23T13:03:40.368 UTC</td>\n",
       "      <td>Secondary pump</td>\n",
       "      <td>inactive</td>\n",
       "      <td>{}</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14428295</th>\n",
       "      <td>ROO-CHI-63Secondary pump</td>\n",
       "      <td>ROO-CHI-63</td>\n",
       "      <td>Rooftop</td>\n",
       "      <td>Chiller system</td>\n",
       "      <td>170101</td>\n",
       "      <td>192.168.170.101</td>\n",
       "      <td>Rooftop\\\\Chiller system\\\\Pumps\\\\Secondary pump...</td>\n",
       "      <td>Pumps</td>\n",
       "      <td>Pumps</td>\n",
       "      <td>2025-05-23T13:00:38.727 UTC</td>\n",
       "      <td>Secondary pump</td>\n",
       "      <td>inactive</td>\n",
       "      <td>{}</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14428296</th>\n",
       "      <td>ROO-CHI-41Primary pump-02</td>\n",
       "      <td>ROO-CHI-41</td>\n",
       "      <td>Rooftop</td>\n",
       "      <td>Chiller system</td>\n",
       "      <td>170101</td>\n",
       "      <td>192.168.170.101</td>\n",
       "      <td>Rooftop\\\\Chiller system\\\\System\\\\Primary pump-...</td>\n",
       "      <td>System</td>\n",
       "      <td>System</td>\n",
       "      <td>2025-05-23T13:00:38.727 UTC</td>\n",
       "      <td>Primary pump-02</td>\n",
       "      <td>active</td>\n",
       "      <td>{}</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14428297 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         service_id  asset_code          site     system_type  \\\n",
       "0              FIR-FCU-01On/Off Cmd  FIR-FCU-01   First Floor             FCU   \n",
       "1              FIR-FCU-01On/Off Cmd  FIR-FCU-01   First Floor             FCU   \n",
       "2              FIR-FCU-01On/Off Cmd  FIR-FCU-01   First Floor             FCU   \n",
       "3              FIR-FCU-01On/Off Cmd  FIR-FCU-01   First Floor             FCU   \n",
       "4              FIR-FCU-01On/Off Cmd  FIR-FCU-01   First Floor             FCU   \n",
       "...                             ...         ...           ...             ...   \n",
       "14428292         GRO-FCU-05Ret temp  GRO-FCU-05  Ground Floor             FCU   \n",
       "14428293         GRO-FCU-05Ret temp  GRO-FCU-05  Ground Floor             FCU   \n",
       "14428294   ROO-CHI-63Secondary pump  ROO-CHI-63       Rooftop  Chiller system   \n",
       "14428295   ROO-CHI-63Secondary pump  ROO-CHI-63       Rooftop  Chiller system   \n",
       "14428296  ROO-CHI-41Primary pump-02  ROO-CHI-41       Rooftop  Chiller system   \n",
       "\n",
       "          device_id        device_ip  \\\n",
       "0            170101  192.168.170.101   \n",
       "1            170101  192.168.170.101   \n",
       "2            170101  192.168.170.101   \n",
       "3            170101  192.168.170.101   \n",
       "4            170101  192.168.170.101   \n",
       "...             ...              ...   \n",
       "14428292     170101  192.168.170.101   \n",
       "14428293     170101  192.168.170.101   \n",
       "14428294     170101  192.168.170.101   \n",
       "14428295     170101  192.168.170.101   \n",
       "14428296     170101  192.168.170.101   \n",
       "\n",
       "                                                object_name equipment_name  \\\n",
       "0                         First Floor\\\\FCU-1F-02 On/Off Cmd      FCU-1F-02   \n",
       "1                         First Floor\\\\FCU-1F-02 On/Off Cmd      FCU-1F-02   \n",
       "2                         First Floor\\\\FCU-1F-02 On/Off Cmd      FCU-1F-02   \n",
       "3                         First Floor\\\\FCU-1F-02 On/Off Cmd      FCU-1F-02   \n",
       "4                         First Floor\\\\FCU-1F-02 On/Off Cmd      FCU-1F-02   \n",
       "...                                                     ...            ...   \n",
       "14428292              Ground Floor\\\\FCU\\\\FCU-GF-05 Ret temp      FCU-GF-05   \n",
       "14428293              Ground Floor\\\\FCU\\\\FCU-GF-05 Ret temp      FCU-GF-05   \n",
       "14428294  Rooftop\\\\Chiller system\\\\Pumps\\\\Secondary pump...          Pumps   \n",
       "14428295  Rooftop\\\\Chiller system\\\\Pumps\\\\Secondary pump...          Pumps   \n",
       "14428296  Rooftop\\\\Chiller system\\\\System\\\\Primary pump-...         System   \n",
       "\n",
       "         equipment_id             data_received_on        datapoint  \\\n",
       "0           FCU-1F-02  2025-08-21T08:11:48.133 UTC       On/Off Cmd   \n",
       "1           FCU-1F-02  2025-08-21T08:08:47.990 UTC       On/Off Cmd   \n",
       "2           FCU-1F-02  2025-08-21T08:05:48.796 UTC       On/Off Cmd   \n",
       "3           FCU-1F-02  2025-08-21T08:02:48.708 UTC       On/Off Cmd   \n",
       "4           FCU-1F-02  2025-08-21T07:59:48.964 UTC       On/Off Cmd   \n",
       "...               ...                          ...              ...   \n",
       "14428292    FCU-GF-05  2025-05-23T12:57:34.673 UTC         Ret temp   \n",
       "14428293    FCU-GF-05  2025-05-23T12:54:28.494 UTC         Ret temp   \n",
       "14428294        Pumps  2025-05-23T13:03:40.368 UTC   Secondary pump   \n",
       "14428295        Pumps  2025-05-23T13:00:38.727 UTC   Secondary pump   \n",
       "14428296       System  2025-05-23T13:00:38.727 UTC  Primary pump-02   \n",
       "\n",
       "             monitoring_data service_status subsystem system_id  \n",
       "0                     active         normal         -         -  \n",
       "1                     active         normal         -         -  \n",
       "2                     active         normal         -         -  \n",
       "3                     active         normal         -         -  \n",
       "4                     active         normal         -         -  \n",
       "...                      ...            ...       ...       ...  \n",
       "14428292   41.72909164428711             {}         -         -  \n",
       "14428293  40.109561920166016             {}         -         -  \n",
       "14428294            inactive             {}         -         -  \n",
       "14428295            inactive             {}         -         -  \n",
       "14428296              active             {}         -         -  \n",
       "\n",
       "[14428297 rows x 15 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd75cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.87157246, 10.01367664,  0.        ,  0.0954916 ,  0.0954916 ,\n",
       "        0.        ,  0.75      ,  1.        ,  0.        ,  0.16543484,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.09090909,  0.75      ,  0.1654348 ,\n",
       "        0.56521739,  0.83333333,  0.        ,  0.08888889,  0.        ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a13799",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3983c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m params\u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m300\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m],\n\u001b[0;32m      3\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m],\n\u001b[0;32m      4\u001b[0m             }\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\debas\\OneDrive\\Desktop\\BMS-AI\\.venv\\lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\debas\\OneDrive\\Desktop\\BMS-AI\\.venv\\lib\\site-packages\\xgboost\\sklearn.py:1247\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1245\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\debas\\OneDrive\\Desktop\\BMS-AI\\.venv\\lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\debas\\OneDrive\\Desktop\\BMS-AI\\.venv\\lib\\site-packages\\xgboost\\training.py:180\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    174\u001b[0m cb_container \u001b[38;5;241m=\u001b[39m CallbackContainer(\n\u001b[0;32m    175\u001b[0m     callbacks, metric\u001b[38;5;241m=\u001b[39mcustom_metric, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcallable\u001b[39m(obj)\n\u001b[0;32m    176\u001b[0m )\n\u001b[0;32m    178\u001b[0m bst \u001b[38;5;241m=\u001b[39m cb_container\u001b[38;5;241m.\u001b[39mbefore_training(bst)\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "params= {\n",
    "                'n_estimators': [100, 200, 300], 'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 5, 7], 'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "            }\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42,**params)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a40bb50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(df : pd.DataFrame = None, file_path: str = 'C:/Users/debas/OneDrive/Desktop/output.csv'):\n",
    "    \"\"\"\n",
    "    Loads data, preprocesses it, and splits it into training and testing sets.\n",
    "    \"\"\"\n",
    "    print(\"--- Loading and Preprocessing Data ---\")\n",
    "    if df is None:\n",
    "        if file_path is None:\n",
    "            raise ValueError(\"Either 'df' or 'file_path' must be provided.\")\n",
    "        df = pd.read_csv(file_path)\n",
    "    \n",
    "    \n",
    "    print(f\"Initial data shape: {df.shape}\")    \n",
    "    df = df[(df['site'] == \"Ground Floor\") & (df['system_type'] == \"AHU\")]\n",
    "    \n",
    "    X, y, preprocessor = transform_data_with_columntransformer(df)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "    \n",
    "    print(f\"Training data shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Testing data shape: X={X_test.shape}, y={y_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15e6de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_with_columntransformer(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    This function takes a pandas DataFrame, applies a series of transformations\n",
    "    using ColumnTransformer, and returns features (X) and target variables (y).\n",
    "    \"\"\"\n",
    "\n",
    "    df['data_received_on'] = pd.to_datetime(df['data_received_on'])\n",
    "    df['data_received_on_naive'] = df['data_received_on'].dt.tz_localize(None)\n",
    "\n",
    "    df.sort_values('data_received_on_naive', inplace=True)\n",
    "\n",
    "    converted_df = df.pivot_table(\n",
    "        index=['data_received_on_naive', 'site', 'system_type'],\n",
    "        columns='datapoint',\n",
    "        values='monitoring_data',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    converted_df.reset_index(inplace=True)\n",
    "\n",
    "    numeric_cols = [\n",
    "        \"RA Damper feedback\", \"SA Pressure setpoint\", \"OA Humid\", \"RA Temp\",\n",
    "        \"RA CO2\", \"RA CO2 setpoint\", \"SA Fan Speed feedback\", \"SA Fan Speed control\",\n",
    "        \"RA Temp control( Valve Feedback)\", \"SA pressure\", \"Fan Power meter (KW)\",\n",
    "        \"RA damper control\", \"OA Temp\", \"OA Flow\", \"SA temp\", \"RA  temperature setpoint\"\n",
    "    ]\n",
    "    present_numeric_cols = [col for col in numeric_cols if col in converted_df.columns]\n",
    "    converted_df[present_numeric_cols] = converted_df[present_numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    cols_to_drop = [\n",
    "        'site', 'system_type', 'Bag filter dirty status', 'Plant enable',\n",
    "        'Trip status', 'airflow Status', 'auto Status', 'pre Filter dirty staus'\n",
    "    ]\n",
    "    converted_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "    if \"Sup fan cmd\" in converted_df.columns:\n",
    "        mappings = {'active': 1, 'inactive': 0}\n",
    "        converted_df[\"Sup fan cmd\"] = converted_df[\"Sup fan cmd\"].replace(mappings)\n",
    "\n",
    "    target_columns = [\"RA damper control\", \"RA Temp control( Valve Feedback)\", \"SA Fan Speed control\", \"Fan Power meter (KW)\"]\n",
    "    present_target_cols = [col for col in target_columns if col in converted_df.columns]\n",
    "    \n",
    "    converted_df.dropna(subset=present_target_cols, inplace=True)\n",
    "    \n",
    "    y = converted_df[present_target_cols]\n",
    "    #X = converted_df.drop(columns=present_target_cols, errors='ignore')\n",
    "    \n",
    "    print(f\"y : {present_target_cols}\")\n",
    "    print(f\"y shape : {y.shape}\")\n",
    "\n",
    "    converted_df['hour'] = converted_df['data_received_on_naive'].dt.hour\n",
    "    converted_df['dayofweek'] = converted_df['data_received_on_naive'].dt.dayofweek\n",
    "    converted_df['month'] = converted_df['data_received_on_naive'].dt.month\n",
    "    converted_df['dayofyear'] = converted_df['data_received_on_naive'].dt.dayofyear\n",
    "    X = converted_df.drop(columns=['data_received_on_naive', 'data_received_on']+present_target_cols, errors='ignore')\n",
    "    \n",
    "    numeric_features = [col for col in X.columns if pd.api.types.is_numeric_dtype(X[col]) and col != 'Sup fan cmd']\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', MinMaxScaler(), numeric_features)\n",
    "        ],\n",
    "        remainder='passthrough'  \n",
    "    )\n",
    "\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "\n",
    "    passthrough_cols = [col for col in X.columns if col not in numeric_features]\n",
    "    transformed_cols = numeric_features + passthrough_cols\n",
    "    X = pd.DataFrame(X_transformed, columns=transformed_cols, index=X.index)\n",
    "\n",
    "\n",
    "    combined = pd.concat([X, y], axis=1)\n",
    "    combined.dropna(inplace=True)\n",
    "    X = combined[X.columns]\n",
    "    \n",
    "    print(f\"y cols : {y.columns}\")\n",
    "    print(f\"combined cols : {combined.columns}\")\n",
    "    y = combined[y.columns]\n",
    "\n",
    "    \n",
    "    return X, y,preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43f4bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataframe(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Helper function to apply the core data transformation logic.\n",
    "    \"\"\"\n",
    "    print(\"Starting data transformation...\")\n",
    "    # 1. Handle timestamps\n",
    "    df['data_received_on'] = pd.to_datetime(df['data_received_on'])\n",
    "    df['data_received_on_naive'] = df['data_received_on'].dt.tz_localize(None)\n",
    "    df.sort_values('data_received_on_naive', inplace=True)\n",
    "\n",
    "    # 2. Pivot the table\n",
    "    converted_df = df.pivot_table(\n",
    "        index=['data_received_on_naive'],\n",
    "        columns='datapoint',\n",
    "        values='monitoring_data',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    converted_df.reset_index(inplace=True)\n",
    "\n",
    "    # 3. Convert data types\n",
    "    numeric_cols = [\n",
    "        \"RA Damper feedback\", \"SA Pressure setpoint\", \"OA Humid\", \"RA Temp\",\n",
    "        \"RA CO2\", \"RA CO2 setpoint\", \"SA Fan Speed feedback\", \"SA Fan Speed control\",\n",
    "        \"RA Temp control( Valve Feedback)\", \"SA pressure\", \"Fan Power meter (KW)\",\n",
    "        \"RA damper control\", \"OA Temp\", \"OA Flow\", \"SA temp\", \"RA  temperature setpoint\"\n",
    "    ]\n",
    "    present_numeric_cols = [col for col in numeric_cols if col in converted_df.columns]\n",
    "    converted_df[present_numeric_cols] = converted_df[present_numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # 4. Map categorical features and handle missing values\n",
    "    if \"Sup fan cmd\" in converted_df.columns:\n",
    "        mappings = {'active': 1, 'inactive': 0}\n",
    "        converted_df[\"Sup fan cmd\"] = converted_df[\"Sup fan cmd\"].replace(mappings).fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "    # 5. Engineer time-based features from the timestamp\n",
    "    print(\"Engineering time-based features...\")\n",
    "    converted_df['hour'] = converted_df['data_received_on_naive'].dt.hour\n",
    "    converted_df['dayofweek'] = converted_df['data_received_on_naive'].dt.dayofweek\n",
    "    converted_df['month'] = converted_df['data_received_on_naive'].dt.month\n",
    "    converted_df['dayofyear'] = converted_df['data_received_on_naive'].dt.dayofyear\n",
    "\n",
    "    # 6. Define target columns and clean data\n",
    "    target_columns = [\"RA damper control\", \"RA Temp control( Valve Feedback)\", \"SA Fan Speed control\", \"Fan Power meter (KW)\"]\n",
    "    present_target_cols = [col for col in target_columns if col in converted_df.columns]\n",
    "\n",
    "    # Drop rows where targets are missing, then fill remaining NaNs\n",
    "    converted_df.dropna(subset=present_target_cols, inplace=True)\n",
    "    converted_df.fillna(method='ffill', inplace=True)\n",
    "    converted_df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # 7. Create final X and y AFTER all engineering and cleaning\n",
    "    y = converted_df[present_target_cols]\n",
    "    # Drop original timestamp and targets to create a clean feature set\n",
    "    X = converted_df.drop(columns=present_target_cols + ['data_received_on_naive'], errors='ignore')\n",
    "\n",
    "    # 8. Create ColumnTransformer for scaling\n",
    "    encoded_categorical_features = []\n",
    "    if 'Sup fan cmd' in X.columns:\n",
    "        encoded_categorical_features.append('Sup fan cmd')\n",
    "\n",
    "    numeric_features = [\n",
    "        col for col in X.columns \n",
    "        if pd.api.types.is_numeric_dtype(X[col]) and col not in encoded_categorical_features\n",
    "    ]\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', MinMaxScaler(), numeric_features),\n",
    "            ('cat', 'passthrough', encoded_categorical_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    transformed_cols = numeric_features + encoded_categorical_features\n",
    "    X = pd.DataFrame(X_transformed, columns=transformed_cols, index=X.index)\n",
    "\n",
    "    print(\"Data transformation complete.\")\n",
    "    # The final alignment/dropna step is no longer needed because of the improved logic\n",
    "    return X, y, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2022e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['site'] == \"Ground Floor\") & (df['system_type'] == \"AHU\")]\n",
    "    \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64170a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data transformation...\n",
      "Engineering time-based features...\n",
      "Data transformation complete.\n"
     ]
    }
   ],
   "source": [
    "X,y,preprocessor = transform_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d64b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc01569c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>datapoint</th>\n",
       "      <th>RA damper control</th>\n",
       "      <th>RA Temp control( Valve Feedback)</th>\n",
       "      <th>SA Fan Speed control</th>\n",
       "      <th>Fan Power meter (KW)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.303454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3643</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.013677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29561</th>\n",
       "      <td>10.0</td>\n",
       "      <td>94.150986</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.792364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27503</th>\n",
       "      <td>10.0</td>\n",
       "      <td>95.241127</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.203832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25909</th>\n",
       "      <td>10.0</td>\n",
       "      <td>95.429337</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.152745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16852</th>\n",
       "      <td>10.0</td>\n",
       "      <td>7.170233</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.345133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.500352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.345133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.032096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15797</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.203832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28860 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "datapoint  RA damper control  RA Temp control( Valve Feedback)  \\\n",
       "374                      0.0                          0.000000   \n",
       "3643                     0.0                          0.000000   \n",
       "29561                   10.0                         94.150986   \n",
       "27503                   10.0                         95.241127   \n",
       "25909                   10.0                         95.429337   \n",
       "...                      ...                               ...   \n",
       "16852                   10.0                          7.170233   \n",
       "6265                     0.0                          0.000000   \n",
       "11284                    0.0                          0.000000   \n",
       "860                      0.0                          0.000000   \n",
       "15797                    0.0                          0.000000   \n",
       "\n",
       "datapoint  SA Fan Speed control  Fan Power meter (KW)  \n",
       "374                         0.0              9.303454  \n",
       "3643                        0.0             10.013677  \n",
       "29561                     100.0              7.792364  \n",
       "27503                     100.0              7.203832  \n",
       "25909                     100.0             10.152745  \n",
       "...                         ...                   ...  \n",
       "16852                     100.0              6.345133  \n",
       "6265                        0.0              9.500352  \n",
       "11284                       0.0              6.345133  \n",
       "860                         0.0              7.032096  \n",
       "15797                       0.0              7.203832  \n",
       "\n",
       "[28860 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328f810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preprocessing Data ---\n",
      "Initial data shape: (14428297, 15)\n",
      "Training data shape: X=(28856, 25), y=(28856, 8)\n",
      "Testing data shape: X=(7215, 25), y=(7215, 8)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, preprocessor =load_and_preprocess_data(df=df,file_path=DATA_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94243514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.87157246, 10.01367664])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "160db1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the MultiOutputRegressor with XGBoost...\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "xgb_regressor = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "multi_output_model = MultiOutputRegressor(estimator=xgb_regressor, n_jobs=-1)\n",
    "\n",
    "print(\"\\nTraining the MultiOutputRegressor with XGBoost...\")\n",
    "multi_output_model.fit(X_train, y_train)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c80d5e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Squared Error on the test set: 1.4658\n"
     ]
    }
   ],
   "source": [
    "y_pred = multi_output_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"\\nMean Squared Error on the test set: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c6798e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of individual estimators (models) trained: 4\n",
      "Shape of predictions: (7215, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nNumber of individual estimators (models) trained: {len(multi_output_model.estimators_)}\")\n",
    "print(f\"Shape of predictions: {y_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def transform_data_with_columntransformer_old(df: pd.DataFrame):\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    This function takes a pandas DataFrame, applies a series of transformations\n",
    "    using ColumnTransformer, and returns features (X) and target variables (y).\n",
    "    \"\"\"\n",
    "\n",
    "    df['data_received_on'] = pd.to_datetime(df['data_received_on'])\n",
    "    df['data_received_on_naive'] = df['data_received_on'].dt.tz_localize(None)\n",
    "\n",
    "    df.sort_values('data_received_on_naive', inplace=True)\n",
    "\n",
    "    converted_df = df.pivot_table(\n",
    "        index=['data_received_on_naive', 'site', 'system_type'],\n",
    "        columns='datapoint',\n",
    "        values='monitoring_data',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    converted_df.reset_index(inplace=True)\n",
    "\n",
    "    numeric_cols = [\n",
    "        \"RA Damper feedback\", \"SA Pressure setpoint\", \"OA Humid\", \"RA Temp\",\n",
    "        \"RA CO2\", \"RA CO2 setpoint\", \"SA Fan Speed feedback\", \"SA Fan Speed control\",\n",
    "        \"RA Temp control( Valve Feedback)\", \"SA pressure\", \"Fan Power meter (KW)\",\n",
    "        \"RA damper control\", \"OA Temp\", \"OA Flow\", \"SA temp\", \"RA  temperature setpoint\"\n",
    "    ]\n",
    "    present_numeric_cols = [col for col in numeric_cols if col in converted_df.columns]\n",
    "    converted_df[present_numeric_cols] = converted_df[present_numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    cols_to_drop = [\n",
    "        'site', 'system_type', 'Bag filter dirty status', 'Plant enable',\n",
    "        'Trip status', 'airflow Status', 'auto Status', 'pre Filter dirty staus'\n",
    "    ]\n",
    "    converted_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "    if \"Sup fan cmd\" in converted_df.columns:\n",
    "        mappings = {'active': 1, 'inactive': 0}\n",
    "        converted_df[\"Sup fan cmd\"] = converted_df[\"Sup fan cmd\"].replace(mappings)\n",
    "\n",
    "    target_columns = [\"RA damper control\", \"RA Temp control( Valve Feedback)\", \"SA Fan Speed control\", \"Fan Power meter (KW)\"]\n",
    "    present_target_cols = [col for col in target_columns if col in converted_df.columns]\n",
    "    \n",
    "    converted_df.dropna(subset=present_target_cols, inplace=True)\n",
    "\n",
    "    y = converted_df[present_target_cols]\n",
    "    X = converted_df.drop(columns=present_target_cols, errors='ignore')\n",
    "\n",
    "    converted_df['hour'] = converted_df['data_received_on_naive'].dt.hour\n",
    "    converted_df['dayofweek'] = converted_df['data_received_on_naive'].dt.dayofweek\n",
    "    converted_df['month'] = converted_df['data_received_on_naive'].dt.month\n",
    "    converted_df['dayofyear'] = converted_df['data_received_on_naive'].dt.dayofyear\n",
    "    X = converted_df.drop(columns=['data_received_on_naive', 'data_received_on'], errors='ignore')\n",
    "    \n",
    "    numeric_features = [col for col in X.columns if pd.api.types.is_numeric_dtype(X[col]) and col != 'Sup fan cmd']\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', MinMaxScaler(), numeric_features)\n",
    "        ],\n",
    "        remainder='passthrough'  \n",
    "    )\n",
    "\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "\n",
    "    passthrough_cols = [col for col in X.columns if col not in numeric_features]\n",
    "    transformed_cols = numeric_features + passthrough_cols\n",
    "    X = pd.DataFrame(X_transformed, columns=transformed_cols, index=X.index)\n",
    "\n",
    "\n",
    "    combined = pd.concat([X, y], axis=1)\n",
    "    combined.dropna(inplace=True)\n",
    "    X = combined[X.columns]\n",
    "    y = combined[y.columns]\n",
    "\n",
    "\n",
    "    return X, y,preprocessor\n",
    "\n",
    "def transform_dataframe(df: pd.DataFrame):\n",
    "    print(f\"{2:-^50}\")\n",
    "    \"\"\"\n",
    "    Helper function to apply the core data transformation logic.\n",
    "    \"\"\"\n",
    "    print(\"Starting data transformation...\")\n",
    "    df['data_received_on'] = pd.to_datetime(df['data_received_on'])\n",
    "    df['data_received_on_naive'] = df['data_received_on'].dt.tz_localize(None)\n",
    "    df.sort_values('data_received_on_naive', inplace=True)\n",
    "\n",
    "    converted_df = df.pivot_table(\n",
    "        index=['data_received_on_naive'],\n",
    "        columns='datapoint',\n",
    "        values='monitoring_data',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    converted_df.reset_index(inplace=True)\n",
    "\n",
    "    numeric_cols = [\n",
    "        \"RA Damper feedback\", \"SA Pressure setpoint\", \"OA Humid\", \"RA Temp\",\n",
    "        \"RA CO2\", \"RA CO2 setpoint\", \"SA Fan Speed feedback\", \"SA Fan Speed control\",\n",
    "        \"RA Temp control( Valve Feedback)\", \"SA pressure\", \"Fan Power meter (KW)\",\n",
    "        \"RA damper control\", \"OA Temp\", \"OA Flow\", \"SA temp\", \"RA  temperature setpoint\"\n",
    "    ]\n",
    "    present_numeric_cols = [col for col in numeric_cols if col in converted_df.columns]\n",
    "    converted_df[present_numeric_cols] = converted_df[present_numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    if \"Sup fan cmd\" in converted_df.columns:\n",
    "        mappings = {'active': 1, 'inactive': 0}\n",
    "        converted_df[\"Sup fan cmd\"] = converted_df[\"Sup fan cmd\"].replace(mappings).fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Engineering time-based features...\")\n",
    "    converted_df['hour'] = converted_df['data_received_on_naive'].dt.hour\n",
    "    converted_df['dayofweek'] = converted_df['data_received_on_naive'].dt.dayofweek\n",
    "    converted_df['month'] = converted_df['data_received_on_naive'].dt.month\n",
    "    converted_df['dayofyear'] = converted_df['data_received_on_naive'].dt.dayofyear\n",
    "\n",
    "    target_columns = [\"RA damper control\", \"RA Temp control( Valve Feedback)\", \"SA Fan Speed control\", \"Fan Power meter (KW)\"]\n",
    "    present_target_cols = [col for col in target_columns if col in converted_df.columns]\n",
    "\n",
    "    converted_df.dropna(subset=present_target_cols, inplace=True)\n",
    "    converted_df.fillna(method='ffill', inplace=True)\n",
    "    converted_df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    y = converted_df[present_target_cols]\n",
    "    X = converted_df.drop(columns=present_target_cols + ['data_received_on_naive'], errors='ignore')\n",
    "\n",
    "    encoded_categorical_features = []\n",
    "    if 'Sup fan cmd' in X.columns:\n",
    "        encoded_categorical_features.append('Sup fan cmd')\n",
    "\n",
    "    numeric_features = [\n",
    "        col for col in X.columns \n",
    "        if pd.api.types.is_numeric_dtype(X[col]) and col not in encoded_categorical_features\n",
    "    ]\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', MinMaxScaler(), numeric_features),\n",
    "            ('cat', 'passthrough', encoded_categorical_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    transformed_cols = numeric_features + encoded_categorical_features\n",
    "    X = pd.DataFrame(X_transformed, columns=transformed_cols, index=X.index)\n",
    "\n",
    "    print(\"Data transformation complete.\")\n",
    "    return X, y, preprocessor\n",
    "\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(df : pd.DataFrame = None, file_path: str = 'C:/Users/debas/OneDrive/Desktop/output.csv'):\n",
    "    print(f\"{1:-^50}\")\n",
    "    \"\"\"\n",
    "    Loads data, preprocesses it, and splits it into training and testing sets.\n",
    "    \"\"\"\n",
    "    print(\"--- Loading and Preprocessing Data ---\")\n",
    "    if df is None:\n",
    "        if file_path is None:\n",
    "            raise ValueError(\"Either 'df' or 'file_path' must be provided.\")\n",
    "        df = pd.read_csv(file_path)\n",
    "    \n",
    "    \n",
    "    print(f\"Initial data shape: {df.shape}\")    \n",
    "    df = df[(df['site'] == \"Ground Floor\") & (df['system_type'] == \"AHU\")]\n",
    "    \n",
    "    X, y, preprocessor = transform_dataframe(df)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    \n",
    "    print(f\"Training data shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Testing data shape: X={X_test.shape}, y={y_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor\n",
    "\n",
    "\n",
    "def tune_sklearn_models(X_train, y_train):\n",
    "    print(f\"{3:-^50}\")\n",
    "    \"\"\"\n",
    "    Defines and tunes scikit-learn models using RandomizedSearchCV.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Training for Scikit-learn Models ---\")\n",
    "    models_to_tune = {\n",
    "        'RandomForest': {\n",
    "            'estimator': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300], 'max_depth': [10, 20, 30, None],\n",
    "                'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'estimator': GradientBoostingRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]\n",
    "            }\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'estimator': xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300], 'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 5, 7], 'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_sklearn_models = {}\n",
    "    for name, config in models_to_tune.items():\n",
    "        print(f\"\\nTuning {name}...\")\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=config['estimator'], param_distributions=config['params'],\n",
    "            n_iter=10, cv=3, verbose=1, random_state=42, n_jobs=-1\n",
    "        )\n",
    "        search_wrapper = MultiOutputRegressor(random_search)\n",
    "        search_wrapper.fit(X_train, y_train)\n",
    "        best_sklearn_models[name] = search_wrapper\n",
    "        print(f\"Finished tuning {name}.\")\n",
    "    \n",
    "    return best_sklearn_models\n",
    "\n",
    "def create_keras_model_builder(input_shape, output_shape):\n",
    "    \"\"\"\n",
    "    Factory function to create the Keras model builder with specific input/output shapes.\n",
    "    \"\"\"\n",
    "    def build_model(hp):\n",
    "        inputs = keras.Input(shape=(input_shape,))\n",
    "        x = inputs\n",
    "        for i in range(hp.Int('num_layers', 1, 3)):\n",
    "            x = layers.Dense(\n",
    "                units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),\n",
    "                activation=hp.Choice('activation', ['relu', 'tanh'])\n",
    "            )(x)\n",
    "            x = layers.Dropout(hp.Float('dropout', 0, 0.5, step=0.1))(x)\n",
    "        outputs = layers.Dense(output_shape)(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=\"mean_squared_error\",\n",
    "            metrics=[\"mean_absolute_error\",\"mean_squared_error\"],\n",
    "        )\n",
    "        return model\n",
    "    return build_model\n",
    "\n",
    "def tune_keras_model(X_train, y_train):\n",
    "    print(f\"{4:-^50}\")\n",
    "    \"\"\"\n",
    "    Tunes and trains a deep learning model using Keras Tuner.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Training for Deep Learning Model ---\")\n",
    "    model_builder = create_keras_model_builder(X_train.shape[1], y_train.shape[1])\n",
    "    tuner = kt.RandomSearch(\n",
    "        model_builder, objective='val_loss', max_trials=10, executions_per_trial=2,\n",
    "        directory='keras_tuner_dir', project_name='multi_output_regression'\n",
    "    )\n",
    "    tuner.search_space_summary()\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    print(\"\\nRunning Keras Tuner search...\")\n",
    "    tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    keras_model = tuner.get_best_models(num_models=1)[0]\n",
    "    print(f\"\\nOptimal Keras hyperparameters found: Learning Rate={best_hps.get('lr'):.4f}\")\n",
    "    return keras_model\n",
    "\n",
    "\n",
    "def evaluate_models(models: dict, X_test, y_test):\n",
    "    print(f\"{5:-^50}\")\n",
    "    \"\"\"\n",
    "    Evaluates a dictionary of trained models on the test set and returns a results DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Evaluating All Models on Test Set ---\")\n",
    "    evaluation_results = {}\n",
    "    for name, model in models.items():\n",
    "        predictions = model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        evaluation_results[name] = {'MAE': mae, 'MSE': mse, 'R2 Score': r2}\n",
    "        print(f\"\\n{name} Metrics: MAE={mae:.4f}, MSE={mse:.4f}, R2 Score={r2:.4f}\")\n",
    "        \n",
    "    results_df = pd.DataFrame(evaluation_results).T\n",
    "    print(\"\\n--- Model Comparison ---\")\n",
    "    print(results_df)\n",
    "    return results_df\n",
    "\n",
    "def save_best_model(best_model_name, best_model, preprocessor):\n",
    "    print(f\"{6:-^50}\")\n",
    "    \"\"\"\n",
    "    Saves the best performing model and its preprocessor to disk.\n",
    "    \"\"\"\n",
    "    print(f\"\\nBest performing model is: {best_model_name}\")\n",
    "    if 'Keras' in best_model_name:\n",
    "        best_model.save('best_model.keras')\n",
    "        joblib.dump(preprocessor, 'preprocessor.joblib')\n",
    "        print(\"Saved Keras model to 'best_model.keras' and preprocessor to 'preprocessor.joblib'\")\n",
    "    else:\n",
    "        pipeline_to_save = {'preprocessor': preprocessor, 'model': best_model}\n",
    "        joblib.dump(pipeline_to_save, f'best_model_{best_model_name}.joblib')\n",
    "        print(f\"Saved complete pipeline to 'best_model_{best_model_name}.joblib'\")\n",
    "\n",
    "\n",
    "def main(df : pd.DataFrame = None, file_path: str = 'C:/Users/debas/OneDrive/Desktop/output.csv'):\n",
    "    print(f\"{0:-^50}\")\n",
    "    \"\"\"\n",
    "    Main function to run the entire training pipeline.\n",
    "    \"\"\"\n",
    "    print(\"Dataframe shape before processing:\", df.shape if df is not None else \"No dataframe provided\")\n",
    "    X_train, X_test, y_train, y_test, preprocessor = load_and_preprocess_data(df=df,file_path=file_path)\n",
    "\n",
    "    print(f\"{X_train.shape=}\")\n",
    "    print(f\"{y_train.shape=}\")\n",
    "    print(f\"{X_test.shape=}\")\n",
    "    print(f\"{y_test.shape=}\") \n",
    "    best_sklearn_models = tune_sklearn_models(X_train, y_train)\n",
    "    keras_model = tune_keras_model(X_train, y_train)\n",
    "\n",
    "    all_models = {**best_sklearn_models, 'Keras_Functional_API': keras_model}\n",
    "    results_df = evaluate_models(all_models, X_test, y_test)\n",
    "\n",
    "    best_model_name = results_df['R2 Score'].idxmax()\n",
    "    best_model = all_models[best_model_name]\n",
    "    save_best_model(best_model_name, best_model, preprocessor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61f8443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------0-------------------------\n",
      "Dataframe shape before processing: No dataframe provided\n",
      "------------------------1-------------------------\n",
      "--- Loading and Preprocessing Data ---\n",
      "Initial data shape: (14428297, 15)\n",
      "------------------------2-------------------------\n",
      "Starting data transformation...\n",
      "Engineering time-based features...\n",
      "Data transformation complete.\n",
      "Training data shape: X=(28860, 17), y=(28860, 4)\n",
      "Testing data shape: X=(7215, 17), y=(7215, 4)\n",
      "X_train.shape=(28860, 17)\n",
      "y_train.shape=(28860, 4)\n",
      "X_test.shape=(7215, 17)\n",
      "y_test.shape=(7215, 4)\n",
      "------------------------3-------------------------\n",
      "\n",
      "--- Starting Training for Scikit-learn Models ---\n",
      "\n",
      "Tuning RandomForest...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Finished tuning RandomForest.\n",
      "\n",
      "Tuning GradientBoosting...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Finished tuning GradientBoosting.\n",
      "\n",
      "Tuning XGBoost...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Finished tuning XGBoost.\n",
      "------------------------4-------------------------\n",
      "\n",
      "--- Starting Training for Deep Learning Model ---\n",
      "Reloading Tuner from keras_tuner_dir\\multi_output_regression\\tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 7\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': 'linear'}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
      "dropout (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "lr (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "units_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "\n",
      "Running Keras Tuner search...\n",
      "WARNING:tensorflow:From c:\\Users\\debas\\OneDrive\\Desktop\\BMS-AI\\.venv\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "\n",
      "Optimal Keras hyperparameters found: Learning Rate=0.0009\n",
      "------------------------5-------------------------\n",
      "\n",
      "--- Evaluating All Models on Test Set ---\n",
      "\n",
      "RandomForest Metrics: MAE=0.0381, MSE=0.8165, R2 Score=0.9992\n",
      "\n",
      "GradientBoosting Metrics: MAE=0.0744, MSE=0.7459, R2 Score=0.9993\n",
      "\n",
      "XGBoost Metrics: MAE=0.1043, MSE=1.0703, R2 Score=0.9991\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\n",
      "Keras_Functional_API Metrics: MAE=0.1820, MSE=0.8986, R2 Score=0.9959\n",
      "\n",
      "--- Model Comparison ---\n",
      "                           MAE       MSE  R2 Score\n",
      "RandomForest          0.038142  0.816489  0.999245\n",
      "GradientBoosting      0.074356  0.745929  0.999278\n",
      "XGBoost               0.104274  1.070325  0.999118\n",
      "Keras_Functional_API  0.182022  0.898644  0.995874\n",
      "------------------------6-------------------------\n",
      "\n",
      "Best performing model is: GradientBoosting\n",
      "Saved complete pipeline to 'best_model_GradientBoosting.joblib'\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    'df': None,\n",
    "    'file_path': 'C:/Users/debas/OneDrive/Desktop/output.csv'\n",
    "}\n",
    "main(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12969b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "search_space = {\n",
    "            'RA  temperature setpoint': np.arange(20.0, 27.5, 0.5),\n",
    "            'RA CO2 setpoint': np.arange(500.0, 825.0, 25.0),\n",
    "            'SA Pressure setpoint': np.arange(500.0, 1250.0, 50.0)\n",
    "        }\n",
    "total_combos = 1\n",
    "for feature, values in search_space.items():\n",
    "    total_combos *= len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f658fc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2925"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aee65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
